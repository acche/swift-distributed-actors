{"primaryContentSections":[{"kind":"declarations","declarations":[{"tokens":[{"kind":"keyword","text":"distributed"},{"kind":"text","text":" "},{"kind":"keyword","text":"actor"},{"kind":"text","text":" "},{"kind":"identifier","text":"OpLogDistributedReceptionist"}],"languages":["swift"],"platforms":["macOS"]}]},{"kind":"content","content":[{"anchor":"Intended-usage--Optimization-choices","level":3,"type":"heading","text":"Intended usage \/ Optimization choices"},{"type":"paragraph","inlineContent":[{"type":"text","text":"This Receptionist implementation is optimized towards small to medium clusters (many tens of nodes) with much actor churn,"},{"type":"text","text":" "},{"type":"text","text":"rather than wide (hundreds of nodes) clusters with few registered actors. This implementation is guided by a pragmatic view of"},{"type":"text","text":" "},{"type":"text","text":"how most actor clusters operate, and also in face of the lack of built-in sharding or “virtual namespace” (yet),"},{"type":"text","text":" "},{"type":"text","text":"which would normally be the way to handle millions of actors and tracking their locations (by “sharding” them and"},{"type":"text","text":" "},{"type":"text","text":"moving and tracking them in groups)."}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"The current implementation gives priority to streaming large amounts of refs between peers, without exceeding"},{"type":"text","text":" "},{"type":"text","text":"a maximum batch size (thus avoiding too large messages, which would lead to head of line blocking of other messages,"},{"type":"text","text":" "},{"type":"text","text":"including heartbeats etc)."}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"We will re-consider and re-implement this receptionist given other requirements, or allow selecting its mode of operation"},{"type":"text","text":" "},{"type":"text","text":"in the future. E.g. a CRDT based receptionist would be preferable for wide clusters, as we can then avoid the “origin”"},{"type":"text","text":" "},{"type":"text","text":"peer having to stream all data to all peer receptionists. This approach however risks having to gossip a full state"},{"type":"text","text":" "},{"type":"text","text":"when a new nodes join, or potentially for keys where we are unable to propagate "},{"type":"emphasis","inlineContent":[{"type":"text","text":"just deltas"}]},{"type":"text","text":". We hope that a future"},{"type":"text","text":" "},{"type":"text","text":"CRDT replicator will be smart enough that we can replace this implementation without fear of too-large messages being"},{"type":"text","text":" "},{"type":"text","text":"put on the wire. We can achieve this in a number of ways (chunking full-state sync into a series of deltas anyway)."},{"type":"text","text":" "},{"type":"text","text":"We will revisit this topic as we implement more advanced CRDT replicators."}]},{"anchor":"Protocol","level":3,"type":"heading","text":"Protocol"},{"type":"paragraph","inlineContent":[{"type":"text","text":"The protocol works on sending individual updates (operations, kind of like an operation based CRDT replicator would),"},{"type":"text","text":" "},{"type":"text","text":"rather than replicating “the entire set of actors registered under a key”. The general implementation can be seen as"},{"type":"text","text":" "},{"type":"text","text":"operation-log replication, where each receptionist “owns” its own log and others ask for “replays” of this log once"},{"type":"text","text":" "},{"type":"text","text":"they notice they are behind (i.e. the last sequence nr they observed of the log is older than the latest available)."},{"type":"text","text":" "},{"type":"text","text":"Once other receptionists notice (due to periodic ack gossip carrying “latest observed sequence nrs” of all other receptionists)"},{"type":"text","text":" "},{"type":"text","text":"a receptionist which’s sequence nr is newer than they have observed from it, they initiate a pull by sending an"},{"type":"text","text":" "},{"type":"text","text":"an "},{"type":"codeVoice","code":"ack(until: latestAppliedSeqNrFromTarget)"},{"type":"text","text":". This causes the owner of the data to initiate a replay of the operations"},{"type":"text","text":" "},{"type":"text","text":"from the "},{"type":"codeVoice","code":"ack.until"},{"type":"text","text":" sequence nr."}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"There are a number of optimizations possible here which will be discussed below, though not all are implemented today."},{"type":"text","text":" "},{"type":"text","text":"As usual in such schemes, log compaction becomes an important topic, though today we do not offer advanced compaction yet."}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"The protocol can be visualized identical state machines acting on incoming operation logs (which inherently separate,"},{"type":"text","text":" "},{"type":"text","text":"and not affecting each other’s), like so:"}]},{"type":"codeListing","syntax":null,"code":["                                                    +---------------+","                                                    |  rec B (@2)   | \/\/ Upon receipt of (1), notice that should pull from A,","                                                +-->>  A@3,B@2,C@4  | \/\/ since A@3 < A@10. It would send (ack <A@3,...>) to initiate a pull.","     +--------------+       (1)                 |   +---------------+","     |  rec A (@10) >>---(ack <B@2,...>)--------+","     | A@10,B@2,C@4 <<---(ops [op,op,...] <>)---+   +---------------+","     +--------------+       (2)                 |   |   rec C (@9)  | \/\/ Caused a different `ack` from A, C knows that A is \"behind\",","                                                +--<< A@10,B@2,C@9  | \/\/ and thus replicates its log to A by sending (2) PushOps.","                                                    +---------------+ \/\/ A will confirm and eventually the two peers are in-sync.","","pull == ack - acknowledgements serve also as pull messages, if the ack-ed receptionist sees there is more data to be sent, it does so","","<v> - collection of \"latest known\" seqNr","op  - versioned operation, when applying an op we know up to which `X@n` n seqNr our state has been forwarded to."]},{"anchor":"Observation-Single-writer-property-of-receptionist-logs-as-they-are-always-local-to-a-receptionist","level":4,"type":"heading","text":"Observation: Single-writer property of receptionist logs, as they are always “local to a receptionist”:"},{"type":"paragraph","inlineContent":[{"type":"text","text":"Each receptionist only takes care of “their own” actors (i.e. on their own nodes), thus they are the authoritative"},{"type":"text","text":" "},{"type":"text","text":"single-writer source of any information if a ref is registered or not. This is why we can rely on the 1:1 replaying"},{"type":"text","text":" "},{"type":"text","text":"of events. Only a given node’s receptionist takes care of the local register\/remove, and later on gets pulled for this"},{"type":"text","text":" "},{"type":"text","text":"information by other receptionists – the other peers will never remove an actor “owned” by another node, unless that"},{"type":"text","text":" "},{"type":"text","text":"node’s receptionist tells them to do so. This also means we do not have to watch every single actor that is spread throughout the cluster."}]},{"anchor":"Observation-Fast-streaming-when-needed--periodic-small-seqNr-dissemination","level":4,"type":"heading","text":"Observation: Fast streaming when needed \/ periodic small seqNr dissemination"},{"type":"paragraph","inlineContent":[{"type":"text","text":"It is worth noting that the protocol works effectively in “one mode,” meaning that there is no explicit “streaming from a peer”"},{"type":"text","text":" "},{"type":"text","text":"and later “just gossiping” mode. The same message ("},{"type":"codeVoice","code":"AckOps"},{"type":"text","text":") is used both to cause a pull, confirmation of receipt of a "},{"type":"codeVoice","code":"PushOps"},{"type":"text","text":" as well as just spread observed sequence"},{"type":"text","text":" "},{"type":"text","text":"number observations."}]},{"anchor":"Optimization-Blip-Registration-Replication-Avoidance","level":4,"type":"heading","text":"Optimization: “Blip” Registration Replication Avoidance"},{"type":"paragraph","inlineContent":[{"type":"text","text":"We define a “blip registration” as a registration of an actor, which immediately (or very quickly) after registering"},{"type":"text","text":" "},{"type":"text","text":"terminates. It can be argued it is NOT useful to replicate the very existence of such short lived actor to other peers,"},{"type":"text","text":" "},{"type":"text","text":"as even if they’d act on the "},{"type":"codeVoice","code":"register"},{"type":"text","text":", it’d be immediately followed by "},{"type":"codeVoice","code":"remove"},{"type":"text","text":" and\/or a termination signal."}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"Thanks to the op-log replication scheme, where we gossip pushes in “batch”, we can notice such blips and avoid pushing them"},{"type":"text","text":" "},{"type":"text","text":"completely to other nodes. could also avoid sending to peers “blips” i.e. actors which register and immediately terminate."},{"type":"text","text":" "},{"type":"text","text":"This remains to be debated, but one could argue it is NOT helpful to replicate such short lived ref at all,"},{"type":"text","text":" "},{"type":"text","text":"if we already know it has terminated, thus we can avoid other nodes acting on this ref which they’d immediately"},{"type":"text","text":" "},{"type":"text","text":"be notified has been terminated already."}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"The tradeoff here is that subscribing to a key may not be used to get “total number of actors that ever registered under this key globally.”"},{"type":"text","text":" "},{"type":"text","text":"We believe this is a sensible tradeoff, as receptionists are only about current and live actors, it MAY however cause weird initialization lockups,"},{"type":"text","text":" "},{"type":"text","text":"where we “know” an actor should have spawned somewhere and registered, but we never see it – this pattern seems very fragile though so it seems"},{"type":"text","text":" "},{"type":"text","text":"to make sense to discourage it."}]},{"anchor":"Overall-protocol-outline","level":3,"type":"heading","text":"Overall protocol outline"},{"type":"paragraph","inlineContent":[{"type":"text","text":"The goal of the protocol is to replicate a log of “ops” ("},{"type":"codeVoice","code":"register"},{"type":"text","text":" and "},{"type":"codeVoice","code":"remove"},{"type":"text","text":" operations), among all receptionists within the cluster."},{"type":"text","text":" "},{"type":"text","text":"This is achieved by implementing atomic broadcast in the form of replay and re-delivery (pull\/acknowledgement driven) of these operations."},{"type":"text","text":" "},{"type":"text","text":"The operations are replicated directly from the “origin” receptionist (“the receptionist local to the actors which register with it”),"},{"type":"text","text":" "},{"type":"text","text":"to all other peer receptionists by means of them pulling for the ops, once they realize they are “behind.” Receptionists periodically"},{"type":"text","text":" "},{"type":"text","text":"gossip their “observed sequence numbers”, which is a set of "},{"type":"codeVoice","code":"Receptionist@SequenceNr"},{"type":"text","text":". When a receptionist notices that for a given"},{"type":"text","text":" "},{"type":"text","text":"remote receptionist an observed "},{"type":"codeVoice","code":"seqNr"},{"type":"text","text":" is "},{"type":"emphasis","inlineContent":[{"type":"text","text":"lower"}]},{"type":"text","text":" than it has already "},{"type":"emphasis","inlineContent":[{"type":"text","text":"applied"}]},{"type":"text","text":" to its state, it performs an ack\/pull from that node directly."}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"This protocol requires that peers communicate with the “origin” receptionist of an actor to obtain its registration,"},{"type":"text","text":" "},{"type":"text","text":"and therefore is NOT a full gossip implementation. The gossip is only used to detect that a pull shall be performed,"},{"type":"text","text":" "},{"type":"text","text":"and the pull from thereon happens directly between those peers, which the recipient MAY flow control if it so wanted to; TODO: more detailed flow control rather than just the maxChunk?"}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"Discussion, Upside: This method as upsides which reflect the p2p nature of actor communication, e.g.: Since to obtain a registration of an actor"},{"type":"text","text":" "},{"type":"text","text":"on node "},{"type":"codeVoice","code":"A"},{"type":"text","text":", we must communicate with node "},{"type":"codeVoice","code":"A"},{"type":"text","text":", this means that if we CANNOT for whatever reason communicate with it (e.g. unreachability),"},{"type":"text","text":" "},{"type":"text","text":"even if we got the reference registration, and emitted it to users, it may end up not being useful."}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"Discussion, Downside: Unlike in a full peer to peer gossip replicated receptionist implementation, nodes MUST communicate with the"},{"type":"text","text":" "},{"type":"text","text":"origin receptionist to obtain refs from it. This can cause a “popular node” to get overwhelmed with having to stream to many other"},{"type":"text","text":" "},{"type":"text","text":"nodes its members… In this implementation we considered the tradeoffs and consider smaller clusters but large amounts of actors"},{"type":"text","text":" "},{"type":"text","text":"to be the dominant usage pattern, and thus are less worried about the large fan-out\/in-cast of these ops streams. This may change"},{"type":"text","text":" "},{"type":"text","text":"as we face larger clusters and we’d love to explore a full CRDT based implementation that DOES NOT need to full-state-sync periodically"},{"type":"text","text":" "},{"type":"text","text":"(which is the reason we avoided an CRDT implementation in the first place today, as we would have to perform a full-state sync of a potentially"},{"type":"text","text":" "},{"type":"emphasis","inlineContent":[{"type":"text","text":"very large"}]},{"type":"text","text":" Dictionary<Key: Set<_ActorRef"},{"type":"text","text":">)."}]},{"type":"orderedList","items":[{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"Members form a cluster, each has exactly one well known receptionist"}]}]},{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"Upon joining, the new receptionist introduces itself and pulls, by sending ack(until: 0)"}]}]},{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"All receptionists receive this and start a replay for it, from the "},{"type":"codeVoice","code":"0"}]}]}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"Gossip, of Latest Op SeqNr:"}]},{"type":"unorderedList","items":[{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"Receptionists gossip about their latestSequenceNr (each has a sequence Nr, associated with add\/remove it performs)"}]}]}]},{"type":"orderedList","items":[{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"When a receptionist receives gossip, it looks at the numbers, and if it notices ANY receptionist where it is “behind”,"},{"type":"text","text":" "},{"type":"text","text":"i.e. we know A@4, but gossip claims it is @10, then we send to it "},{"type":"codeVoice","code":"ack(4)"},{"type":"text","text":" which causes the receptionist to reply with"},{"type":"text","text":" "},{"type":"text","text":"its latest information"}]}]}]},{"type":"unorderedList","items":[{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"Pushed information is batched (!), i.e the pushed would send only e.g. 50 updates, yet still inform us with its"},{"type":"text","text":" "},{"type":"text","text":"latest SeqNr, and we’d notice that actually bu now it already is @100, thus we "},{"type":"codeVoice","code":"ack(50)"},{"type":"text","text":", and it continues the sending."},{"type":"text","text":" "},{"type":"text","text":"(Note that we simply always "},{"type":"codeVoice","code":"ack(latest)"},{"type":"text","text":" and if in the meantime the pusher got more updates, it’ll push those to us as well."}]}]}]}]}],"schemaVersion":{"major":0,"minor":3,"patch":0},"sections":[],"variants":[{"paths":["\/documentation\/distributedcluster\/oplogdistributedreceptionist"],"traits":[{"interfaceLanguage":"swift"}]}],"relationshipsSections":[{"identifiers":["doc:\/\/DistributedCluster\/11Distributed0A5ActorP","doc:\/\/DistributedCluster\/documentation\/DistributedCluster\/DistributedReceptionist","doc:\/\/DistributedCluster\/documentation\/DistributedCluster\/LifecycleWatch","doc:\/\/DistributedCluster\/s23CustomStringConvertibleP","doc:\/\/DistributedCluster\/Se","doc:\/\/DistributedCluster\/SE","doc:\/\/DistributedCluster\/SQ","doc:\/\/DistributedCluster\/SH","doc:\/\/DistributedCluster\/s12IdentifiableP","doc:\/\/DistributedCluster\/s8SendableP","doc:\/\/DistributedCluster\/12_Concurrency8AnyActorP"],"kind":"relationships","title":"Conforms To","type":"conformsTo"}],"identifier":{"url":"doc:\/\/DistributedCluster\/documentation\/DistributedCluster\/OpLogDistributedReceptionist","interfaceLanguage":"swift"},"abstract":[{"type":"text","text":"Cluster-aware (atomic broadcast style, push\/pull-gossip) Receptionist implementation."}],"kind":"symbol","metadata":{"fragments":[{"kind":"keyword","text":"class"},{"kind":"text","text":" "},{"kind":"identifier","text":"OpLogDistributedReceptionist"}],"title":"OpLogDistributedReceptionist","roleHeading":"Class","role":"symbol","symbolKind":"class","externalID":"s:18DistributedCluster05OpLogA12ReceptionistC","modules":[{"name":"DistributedCluster"}],"navigatorTitle":[{"kind":"identifier","text":"OpLogDistributedReceptionist"}]},"hierarchy":{"paths":[["doc:\/\/DistributedCluster\/documentation\/DistributedCluster"]]},"topicSections":[{"title":"Instance Methods","identifiers":["doc:\/\/DistributedCluster\/documentation\/DistributedCluster\/OpLogDistributedReceptionist\/listing(of:file:line:)-21q5q"]},{"title":"Type Aliases","identifiers":["doc:\/\/DistributedCluster\/documentation\/DistributedCluster\/OpLogDistributedReceptionist\/ActorSystem"]},{"title":"Default Implementations","identifiers":["doc:\/\/DistributedCluster\/documentation\/DistributedCluster\/OpLogDistributedReceptionist\/DistributedActor-Implementations","doc:\/\/DistributedCluster\/documentation\/DistributedCluster\/OpLogDistributedReceptionist\/DistributedReceptionist-Implementations","doc:\/\/DistributedCluster\/documentation\/DistributedCluster\/OpLogDistributedReceptionist\/Equatable-Implementations","doc:\/\/DistributedCluster\/documentation\/DistributedCluster\/OpLogDistributedReceptionist\/Hashable-Implementations","doc:\/\/DistributedCluster\/documentation\/DistributedCluster\/OpLogDistributedReceptionist\/Identifiable-Implementations","doc:\/\/DistributedCluster\/documentation\/DistributedCluster\/OpLogDistributedReceptionist\/LifecycleWatch-Implementations"],"generated":true}],"references":{"doc://DistributedCluster/documentation/DistributedCluster/LifecycleWatch":{"role":"symbol","title":"LifecycleWatch","fragments":[{"kind":"keyword","text":"protocol"},{"kind":"text","text":" "},{"kind":"identifier","text":"LifecycleWatch"}],"abstract":[{"type":"text","text":"Provides a distributed actor with the ability to “watch” other actors lifecycles."}],"identifier":"doc:\/\/DistributedCluster\/documentation\/DistributedCluster\/LifecycleWatch","kind":"symbol","type":"topic","navigatorTitle":[{"kind":"identifier","text":"LifecycleWatch"}],"url":"\/documentation\/distributedcluster\/lifecyclewatch"},"doc://DistributedCluster/s12IdentifiableP":{"type":"unresolvable","title":"Swift.Identifiable","identifier":"doc:\/\/DistributedCluster\/s12IdentifiableP"},"doc://DistributedCluster/documentation/DistributedCluster/DistributedReceptionist":{"role":"symbol","title":"DistributedReceptionist","fragments":[{"kind":"keyword","text":"protocol"},{"kind":"text","text":" "},{"kind":"identifier","text":"DistributedReceptionist"}],"abstract":[{"type":"text","text":"A receptionist is a system actor that allows users to register actors under"},{"type":"text","text":" "},{"type":"text","text":"a key to make them available to other parts of the system, without having to"},{"type":"text","text":" "},{"type":"text","text":"share a reference with that specific part directly."}],"identifier":"doc:\/\/DistributedCluster\/documentation\/DistributedCluster\/DistributedReceptionist","kind":"symbol","type":"topic","navigatorTitle":[{"kind":"identifier","text":"DistributedReceptionist"}],"url":"\/documentation\/distributedcluster\/distributedreceptionist"},"doc://DistributedCluster/documentation/DistributedCluster/OpLogDistributedReceptionist/Hashable-Implementations":{"role":"collectionGroup","title":"Hashable Implementations","abstract":[],"identifier":"doc:\/\/DistributedCluster\/documentation\/DistributedCluster\/OpLogDistributedReceptionist\/Hashable-Implementations","kind":"article","type":"topic","url":"\/documentation\/distributedcluster\/oplogdistributedreceptionist\/hashable-implementations"},"doc://DistributedCluster/documentation/DistributedCluster/OpLogDistributedReceptionist/ActorSystem":{"role":"symbol","title":"OpLogDistributedReceptionist.ActorSystem","fragments":[{"kind":"keyword","text":"typealias"},{"kind":"text","text":" "},{"kind":"identifier","text":"ActorSystem"}],"abstract":[],"identifier":"doc:\/\/DistributedCluster\/documentation\/DistributedCluster\/OpLogDistributedReceptionist\/ActorSystem","kind":"symbol","type":"topic","navigatorTitle":[{"kind":"identifier","text":"ActorSystem"}],"url":"\/documentation\/distributedcluster\/oplogdistributedreceptionist\/actorsystem"},"doc://DistributedCluster/documentation/DistributedCluster/OpLogDistributedReceptionist/listing(of:file:line:)-21q5q":{"role":"symbol","title":"listing(of:file:line:)","fragments":[{"kind":"keyword","text":"func"},{"kind":"text","text":" "},{"kind":"identifier","text":"listing"},{"kind":"text","text":"<"},{"kind":"genericParameter","text":"Guest"},{"kind":"text","text":">("},{"kind":"externalParam","text":"of"},{"kind":"text","text":": "},{"kind":"typeIdentifier","text":"Guest"},{"kind":"text","text":".Type, "},{"kind":"externalParam","text":"file"},{"kind":"text","text":": "},{"kind":"typeIdentifier","text":"String","preciseIdentifier":"s:SS"},{"kind":"text","text":", "},{"kind":"externalParam","text":"line"},{"kind":"text","text":": "},{"kind":"typeIdentifier","text":"UInt","preciseIdentifier":"s:Su"},{"kind":"text","text":") "},{"kind":"keyword","text":"async"},{"kind":"text","text":" -> "},{"kind":"typeIdentifier","text":"DistributedReception","preciseIdentifier":"s:18DistributedCluster0A9ReceptionO"},{"kind":"text","text":"."},{"kind":"typeIdentifier","text":"GuestListing","preciseIdentifier":"s:18DistributedCluster0A9ReceptionO12GuestListingV"},{"kind":"text","text":"<"},{"kind":"typeIdentifier","text":"Guest"},{"kind":"text","text":">"}],"abstract":[],"identifier":"doc:\/\/DistributedCluster\/documentation\/DistributedCluster\/OpLogDistributedReceptionist\/listing(of:file:line:)-21q5q","kind":"symbol","type":"topic","url":"\/documentation\/distributedcluster\/oplogdistributedreceptionist\/listing(of:file:line:)-21q5q"},"doc://DistributedCluster/SQ":{"type":"unresolvable","title":"Swift.Equatable","identifier":"doc:\/\/DistributedCluster\/SQ"},"doc://DistributedCluster/documentation/DistributedCluster/OpLogDistributedReceptionist":{"role":"symbol","title":"OpLogDistributedReceptionist","fragments":[{"kind":"keyword","text":"class"},{"kind":"text","text":" "},{"kind":"identifier","text":"OpLogDistributedReceptionist"}],"abstract":[{"type":"text","text":"Cluster-aware (atomic broadcast style, push\/pull-gossip) Receptionist implementation."}],"identifier":"doc:\/\/DistributedCluster\/documentation\/DistributedCluster\/OpLogDistributedReceptionist","kind":"symbol","type":"topic","navigatorTitle":[{"kind":"identifier","text":"OpLogDistributedReceptionist"}],"url":"\/documentation\/distributedcluster\/oplogdistributedreceptionist"},"doc://DistributedCluster/s23CustomStringConvertibleP":{"type":"unresolvable","title":"Swift.CustomStringConvertible","identifier":"doc:\/\/DistributedCluster\/s23CustomStringConvertibleP"},"doc://DistributedCluster/Se":{"type":"unresolvable","title":"Swift.Decodable","identifier":"doc:\/\/DistributedCluster\/Se"},"doc://DistributedCluster/documentation/DistributedCluster":{"role":"collection","title":"DistributedCluster","abstract":[{"type":"text","text":"A peer-to-peer cluster actor system implementation for Swift."}],"identifier":"doc:\/\/DistributedCluster\/documentation\/DistributedCluster","kind":"symbol","type":"topic","url":"\/documentation\/distributedcluster"},"doc://DistributedCluster/documentation/DistributedCluster/OpLogDistributedReceptionist/Equatable-Implementations":{"role":"collectionGroup","title":"Equatable Implementations","abstract":[],"identifier":"doc:\/\/DistributedCluster\/documentation\/DistributedCluster\/OpLogDistributedReceptionist\/Equatable-Implementations","kind":"article","type":"topic","url":"\/documentation\/distributedcluster\/oplogdistributedreceptionist\/equatable-implementations"},"doc://DistributedCluster/documentation/DistributedCluster/OpLogDistributedReceptionist/DistributedActor-Implementations":{"role":"collectionGroup","title":"DistributedActor Implementations","abstract":[],"identifier":"doc:\/\/DistributedCluster\/documentation\/DistributedCluster\/OpLogDistributedReceptionist\/DistributedActor-Implementations","kind":"article","type":"topic","url":"\/documentation\/distributedcluster\/oplogdistributedreceptionist\/distributedactor-implementations"},"doc://DistributedCluster/SE":{"type":"unresolvable","title":"Swift.Encodable","identifier":"doc:\/\/DistributedCluster\/SE"},"doc://DistributedCluster/documentation/DistributedCluster/OpLogDistributedReceptionist/LifecycleWatch-Implementations":{"role":"collectionGroup","title":"LifecycleWatch Implementations","abstract":[],"identifier":"doc:\/\/DistributedCluster\/documentation\/DistributedCluster\/OpLogDistributedReceptionist\/LifecycleWatch-Implementations","kind":"article","type":"topic","url":"\/documentation\/distributedcluster\/oplogdistributedreceptionist\/lifecyclewatch-implementations"},"doc://DistributedCluster/documentation/DistributedCluster/OpLogDistributedReceptionist/Identifiable-Implementations":{"role":"collectionGroup","title":"Identifiable Implementations","abstract":[],"identifier":"doc:\/\/DistributedCluster\/documentation\/DistributedCluster\/OpLogDistributedReceptionist\/Identifiable-Implementations","kind":"article","type":"topic","url":"\/documentation\/distributedcluster\/oplogdistributedreceptionist\/identifiable-implementations"},"doc://DistributedCluster/12_Concurrency8AnyActorP":{"type":"unresolvable","title":"_Concurrency.AnyActor","identifier":"doc:\/\/DistributedCluster\/12_Concurrency8AnyActorP"},"doc://DistributedCluster/11Distributed0A5ActorP":{"type":"unresolvable","title":"Distributed.DistributedActor","identifier":"doc:\/\/DistributedCluster\/11Distributed0A5ActorP"},"doc://DistributedCluster/documentation/DistributedCluster/OpLogDistributedReceptionist/DistributedReceptionist-Implementations":{"role":"collectionGroup","title":"DistributedReceptionist Implementations","abstract":[],"identifier":"doc:\/\/DistributedCluster\/documentation\/DistributedCluster\/OpLogDistributedReceptionist\/DistributedReceptionist-Implementations","kind":"article","type":"topic","url":"\/documentation\/distributedcluster\/oplogdistributedreceptionist\/distributedreceptionist-implementations"},"doc://DistributedCluster/s8SendableP":{"type":"unresolvable","title":"Swift.Sendable","identifier":"doc:\/\/DistributedCluster\/s8SendableP"},"doc://DistributedCluster/SH":{"type":"unresolvable","title":"Swift.Hashable","identifier":"doc:\/\/DistributedCluster\/SH"}}}